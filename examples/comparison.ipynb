{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\n",
      "=== valve1-1 === [405.0/1094]==================================================\n",
      "\n",
      "===== Local Outlier Factor ====================================================\n",
      "{'LocalOutlierFactor': {'n_neighbors': 126}, 'QuantileFilter': {'q': 0.42452610189521156}}\n",
      "Avg. latency per sample: 97.78022962252881ms\n",
      "Pred anomalous samples | events | proportion:           744      | 353   | 68.01%\n",
      "Found samples | events | proportion:                    369      |       | 91.11%\n",
      "\n",
      "=== valve1-2 === [403.0/1154]==================================================\n",
      "\n",
      "===== Local Outlier Factor ====================================================\n",
      "{'LocalOutlierFactor': {'n_neighbors': 57}, 'QuantileFilter': {'q': 0.41149110391990545}}\n",
      "Avg. latency per sample: 34.37178221691006ms\n",
      "Pred anomalous samples | events | proportion:           881      | 317   | 76.34%\n",
      "Found samples | events | proportion:                    376      |       | 93.30%\n",
      "\n",
      "=== valve1-3 === [349.0/1095]==================================================\n",
      "\n",
      "===== Local Outlier Factor ====================================================\n",
      "{'LocalOutlierFactor': {'n_neighbors': 124}, 'QuantileFilter': {'q': 0.3165671934646726}}\n",
      "Avg. latency per sample: 100.57390966371858ms\n",
      "Pred anomalous samples | events | proportion:           738      | 395   | 67.40%\n",
      "Found samples | events | proportion:                    293      |       | 83.95%\n",
      "\n",
      "=== valve1-4 === [401.0/1147]==================================================\n",
      "\n",
      "===== Local Outlier Factor ====================================================\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bayes_opt import (\n",
    "    BayesianOptimization,\n",
    "    SequentialDomainReductionTransformer,\n",
    ")\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from river import anomaly, preprocessing\n",
    "from river.decomposition import OnlineDMD\n",
    "from river.metrics import F1\n",
    "from river.utils import Rolling\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sys.path.insert(1, str(Path().resolve().parent))\n",
    "from functions.datasets import load_skab  # noqa: E402\n",
    "from functions.compose import build_model, convert_to_nested_dict  # noqa: E402\n",
    "from functions.chdsubid import DMDOptSubIDChangeDetector  # noqa: E402\n",
    "from functions.evaluate import (  # noqa: E402\n",
    "    build_fit_evaluate,\n",
    "    print_stats,\n",
    "    progressive_val_predict,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# CONSTANTS\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "\n",
    "# FUNCTIONS\n",
    "def get_random_samples(df: pd.DataFrame, num_samples=10000):\n",
    "    if len(df) <= num_samples:\n",
    "        return df\n",
    "    else:\n",
    "        return df.sample(n=num_samples, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def plot_detection(df: pd.DataFrame, y_pred):\n",
    "    df[\"pred\"] = y_pred\n",
    "    if \"anomaly\" in df.columns:\n",
    "        df = get_random_samples(df)\n",
    "        if len(df.columns) >= 4:\n",
    "            # Separate the feature columns from the target column (\"anomaly\")\n",
    "            X = df.drop(columns=[\"anomaly\", \"pred\"])\n",
    "            y = df[\"anomaly\"]\n",
    "            y_pred = df[\"pred\"]\n",
    "\n",
    "            # Apply PCA to reduce the feature columns to 2 components\n",
    "            pca = PCA(n_components=2)\n",
    "            X_pca = pca.fit_transform(X)\n",
    "\n",
    "            # Create a new DataFrame with the reduced components and \"anomaly\" column\n",
    "            df_pca = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\n",
    "            df_pca[\"anomaly\"] = y.values\n",
    "            df_pca[\"pred\"] = y_pred.values\n",
    "        else:\n",
    "            print(True)\n",
    "            df_pca = pd.DataFrame(df.reset_index().copy())\n",
    "            df_pca.columns = [\"PC1\", \"PC2\", \"anomaly\", \"pred\"]\n",
    "\n",
    "        # Plot the 2D scatter plot\n",
    "        plt.scatter(\n",
    "            df_pca[df_pca[\"anomaly\"] == 0][\"PC1\"],\n",
    "            df_pca[df_pca[\"anomaly\"] == 0][\"PC2\"],\n",
    "        )\n",
    "        plt.scatter(\n",
    "            df_pca[df_pca[\"anomaly\"] == 1][\"PC1\"],\n",
    "            df_pca[df_pca[\"anomaly\"] == 1][\"PC2\"],\n",
    "            facecolors=\"none\",\n",
    "            edgecolors=\"r\",\n",
    "            linewidths=0.5,\n",
    "        )\n",
    "        plt.scatter(\n",
    "            df_pca[df_pca[\"pred\"] == 1][\"PC1\"],\n",
    "            df_pca[df_pca[\"pred\"] == 1][\"PC2\"],\n",
    "            marker=\"x\",  # type: ignore\n",
    "            linewidths=1,\n",
    "        )  # type: ignore\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "\n",
    "def save_results_y(df_ys, path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    name = \"ys\"\n",
    "    if os.path.exists(f\"{path}/ys.csv\"):\n",
    "        try:\n",
    "            df_ys = pd.concat(\n",
    "                [pd.read_csv(f\"{path}/ys.csv\"), df_ys], axis=1, sort=False\n",
    "            )\n",
    "        except:\n",
    "            from datetime import datetime\n",
    "\n",
    "            name = f\"ys_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "    df_ys.to_csv(f\"{path}/{name}.csv\", index=False)\n",
    "\n",
    "\n",
    "def save_results_metrics(metrics_res, path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    metrics_res.to_csv(f\"{path}/metrics.csv\")\n",
    "\n",
    "\n",
    "# MODS\n",
    "class QuantileFilter(anomaly.QuantileFilter):\n",
    "    def __init__(\n",
    "        self, anomaly_detector, q: float, protect_anomaly_detector=True\n",
    "    ):\n",
    "        super().__init__(\n",
    "            anomaly_detector=anomaly_detector,\n",
    "            protect_anomaly_detector=protect_anomaly_detector,\n",
    "            q=q,\n",
    "        )\n",
    "\n",
    "    def predict_one(self, *args):\n",
    "        score = self.score_one(*args)\n",
    "        return self.classify(score)\n",
    "\n",
    "\n",
    "# SETTINGS\n",
    "\n",
    "# DETECTION ALGORITHMS\n",
    "detection_algorithms = [\n",
    "    # (\n",
    "    #     \"SubID Change Detector\",\n",
    "    #     [\n",
    "    #         preprocessing.Hankelizer,\n",
    "    #         [DMDOptSubIDChangeDetector, [Rolling, OnlineDMD]],\n",
    "    #     ],\n",
    "    #     {\n",
    "    #         \"Hankelizer__w__round\": (1, 30),\n",
    "    #         \"Rolling__window_size__round\": (50, 10000),\n",
    "    #         \"DMDOptSubIDChangeDetector__ref_size__round\": (50, 400),\n",
    "    #         \"DMDOptSubIDChangeDetector__test_size__round\": (50, 400),\n",
    "    #         \"DMDOptSubIDChangeDetector__threshold\": (0.0, 100),\n",
    "    #         \"OnlineDMD__r__round\": (2, 6),\n",
    "    #         \"OnlineDMD__eig_rtol\": (0, 1e1),\n",
    "    #     },\n",
    "    # # ),\n",
    "    (\n",
    "        \"Local Outlier Factor\",\n",
    "        [[QuantileFilter, anomaly.LocalOutlierFactor]],\n",
    "        {\n",
    "            \"QuantileFilter__q\": (0.001, 0.999),\n",
    "            \"LocalOutlierFactor__n_neighbors__round\": (0, 200),\n",
    "        },\n",
    "    ),\n",
    "    # (\n",
    "    #     \"One-Class SVM\",\n",
    "    #     [preprocessing.StandardScaler, [QuantileFilter, anomaly.OneClassSVM]],\n",
    "    #     {\n",
    "    #         \"QuantileFilter__q\": (0.001, 0.999),\n",
    "    #         \"OneClassSVM__intercept_lr\": (0.005, 0.02),\n",
    "    #     },\n",
    "    # ),\n",
    "    # (\n",
    "    #     \"Half-Space Trees\",\n",
    "    #     [preprocessing.MinMaxScaler, [QuantileFilter, anomaly.HalfSpaceTrees]],\n",
    "    #     {\n",
    "    #         \"QuantileFilter__q\": (0.001, 0.999),\n",
    "    #         \"HalfSpaceTrees__n_trees__round\": (1, 20),\n",
    "    #         \"HalfSpaceTrees__height__round\": (1, 16),\n",
    "    #         \"HalfSpaceTrees__window_size__round\": (100, 400),\n",
    "    #     },\n",
    "    # ),\n",
    "]\n",
    "\n",
    "# DATASETS\n",
    "# Read data\n",
    "dfs = load_skab(\"data/skab\")\n",
    "dfs_free = [\n",
    "    df\n",
    "    for key, df_group in dfs.items()\n",
    "    for df in df_group\n",
    "    if key == \"anomaly-free\"\n",
    "]\n",
    "datasets: list[dict] = [\n",
    "    {\n",
    "        \"name\": f\"{key}-{i}\",\n",
    "        \"data\": pd.DataFrame(df),\n",
    "        \"anomaly_col\": \"anomaly\",\n",
    "        \"drop\": \"changepoint\",\n",
    "    }\n",
    "    for key, df_group in dfs.items()\n",
    "    for i, df in enumerate(df_group)\n",
    "    if key != \"anomaly-free\"\n",
    "][1:]\n",
    "for df in datasets:\n",
    "    df[\"data\"].index = pd.to_datetime(df[\"data\"].index)\n",
    "\n",
    "# PLOT CONFIG\n",
    "plt.figure(figsize=(len([1, 1, 1]) * 2 + 4, 12.5))\n",
    "plt.subplots_adjust(\n",
    "    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n",
    ")\n",
    "plot_num = 1\n",
    "\n",
    "# RUN\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for dataset in datasets:\n",
    "        # PREPROCESS DATA\n",
    "        df = dataset[\"data\"]\n",
    "        df.index = pd.to_timedelta(\n",
    "            range(0, len(df)), \"T\"\n",
    "        ) + pd.Timestamp.utcnow().replace(microsecond=0)\n",
    "        if isinstance(dataset[\"anomaly_col\"], str):\n",
    "            df = df.rename(columns={dataset[\"anomaly_col\"]: \"anomaly\"})\n",
    "        elif isinstance(dataset[\"anomaly_col\"], pd.Series):\n",
    "            df_y = dataset[\"anomaly_col\"]\n",
    "            df[\"anomaly\"] = df_y.rename(\"anomaly\").values\n",
    "        if dataset[\"drop\"] is not None:\n",
    "            df = df.drop(columns=dataset[\"drop\"])\n",
    "        print(\n",
    "            f\"\\n=== {dataset['name']} === [{sum(df['anomaly'])}/{len(df)}]\".ljust(\n",
    "                80, \"=\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        df_ys = df[[\"anomaly\"]].copy()\n",
    "        # RUN EACH MODEL AGAINST DATASET\n",
    "        for alg in detection_algorithms:\n",
    "            print(f\"\\n===== {alg[0]} \".ljust(80, \"=\"))\n",
    "            # INITIALIZE OPTIMIZER\n",
    "            pbounds = alg[2]\n",
    "            mod_fun = partial(\n",
    "                build_fit_evaluate,\n",
    "                alg[1],\n",
    "                df,\n",
    "                F1(),\n",
    "                map_cluster_to_rc=False,\n",
    "                drop_no_support=False,\n",
    "            )\n",
    "\n",
    "            # TUNE HYPERPARAMETERS\n",
    "            optimizer = BayesianOptimization(\n",
    "                f=mod_fun,\n",
    "                pbounds=pbounds,\n",
    "                random_state=RANDOM_STATE,\n",
    "                verbose=2,\n",
    "                # bounds_transformer=SequentialDomainReductionTransformer(\n",
    "                #     minimum_window=0.5\n",
    "                # ),\n",
    "                allow_duplicate_points=True,\n",
    "            )\n",
    "            logger = JSONLogger(\n",
    "                path=f\"./resultslof/{dataset['name']}-{alg[0]}.log\"\n",
    "            )\n",
    "            optimizer.subscribe(Events.OPTIMIZATION_END, logger)\n",
    "            optimizer.maximize(init_points=50, n_iter=200)\n",
    "            params = convert_to_nested_dict(optimizer.max[\"params\"])\n",
    "            print(params)\n",
    "            model = build_model(alg[1], params)\n",
    "            if hasattr(model, \"seed\"):\n",
    "                model.seed = RANDOM_STATE  # type: ignore\n",
    "            if hasattr(model, \"random_state\"):\n",
    "                model.random_state = RANDOM_STATE  # type: ignore\n",
    "            # USE TUNED MODEL\n",
    "            # PROGRESSIVE PREDICT\n",
    "            y_pred, _ = progressive_val_predict(model, df, metrics=None)\n",
    "\n",
    "            # SAVE PREDICITONS\n",
    "            df_ys[f\"{alg[0]}__{params}\"] = y_pred\n",
    "\n",
    "            #  PRINT OUT LAST DETECTION RESULTS\n",
    "            print_stats(df, y_pred)\n",
    "            plt.subplot(len(datasets), len(detection_algorithms), plot_num)\n",
    "            plot_detection(df, y_pred)\n",
    "            plot_num += 1\n",
    "\n",
    "        # LOAD RESULTS\n",
    "        #  Save\n",
    "        dir_path = f\"resultslof/{dataset['name']}\"\n",
    "        save_results_y(df_ys, f\"resultslof/{dataset['name']}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from river.metrics import (\n",
    "    F1,\n",
    "    Precision,\n",
    "    Recall,\n",
    "    ROCAUC,\n",
    "    RollingROCAUC,\n",
    "    ClassificationReport,\n",
    ")\n",
    "from river.stats import Mean\n",
    "\n",
    "sys.path.insert(1, str(Path().resolve().parent))\n",
    "from functions.evaluate import batch_save_evaluate_metrics\n",
    "\n",
    "\n",
    "class MeanRollingROCAUC(RollingROCAUC):\n",
    "    def __init__(self, window_size=1000, pos_val=True):\n",
    "        super().__init__(window_size, pos_val)\n",
    "        self.mean = Mean()\n",
    "\n",
    "    def update(self, y_true, y_pred):\n",
    "        super().update(y_true, y_pred)\n",
    "        self.mean.update(super().get())\n",
    "        return self\n",
    "\n",
    "    def get(self):\n",
    "        return self.mean.get()\n",
    "\n",
    "\n",
    "metrics = [\n",
    "    Precision(),\n",
    "    Recall(),\n",
    "    F1(),\n",
    "    ROCAUC(),\n",
    "    MeanRollingROCAUC(),\n",
    "    ClassificationReport(),\n",
    "]\n",
    "\n",
    "path = \".results/\"\n",
    "\n",
    "batch_save_evaluate_metrics(metrics, path, task=\"classification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
